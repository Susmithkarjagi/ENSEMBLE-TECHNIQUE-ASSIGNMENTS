{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75fa95ee-26c6-4b3a-a8b3-35b5dbcb8dcc",
   "metadata": {},
   "source": [
    "## ENSEMBLE TECHNIQUE ASSIGNMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fbea0d-e9d8-43a6-b379-19605a55e6fd",
   "metadata": {},
   "source": [
    "Q1. What is an ensemble technique in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d563e75-193c-4131-9113-0e46a7433ff7",
   "metadata": {},
   "source": [
    "In machine learning, an ensemble technique refers to the combination of multiple individual models to improve the overall predictive performance and robustness. Instead of relying on a single model, an ensemble leverages the collective wisdom of multiple models to make more accurate predictions.\n",
    "\n",
    "Ensemble techniques work on the principle of \"wisdom of the crowd,\" where different models may have their strengths and weaknesses, and by combining them, we can mitigate their individual limitations and achieve better results.\n",
    "\n",
    "There are several popular ensemble techniques, including:\n",
    "\n",
    "Bagging: Bagging, short for bootstrap aggregating, involves training multiple instances of the same base model on different subsets of the training data. The predictions from these models are then combined, usually by averaging or voting, to obtain the final prediction. Random Forest is an example of a bagging ensemble algorithm.\n",
    "\n",
    "Boosting: Boosting works by sequentially training multiple weak models, where each subsequent model focuses on improving the mistakes made by the previous models. The predictions from these weak models are combined to produce the final prediction. Gradient Boosting Machines (GBM), AdaBoost, and XGBoost are popular boosting algorithms.\n",
    "\n",
    "Stacking: Stacking, also known as stacked generalization, involves training multiple diverse models and using a meta-model to combine their predictions. The diverse models can be based on different algorithms or variations of the same algorithm with different hyperparameters. The meta-model learns how to weigh the predictions from different models to make the final prediction.\n",
    "\n",
    "Voting: Voting ensembles combine predictions from multiple models by using a voting scheme, such as majority voting or weighted voting, to determine the final prediction. It can be used with any type of model and is particularly useful when the models have similar predictive performance.\n",
    "\n",
    "Ensemble techniques are widely used because they can often improve the accuracy, stability, and generalization of machine learning models. By leveraging the diversity and collective knowledge of multiple models, ensembles can better handle complex patterns in data and reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5188b089-3394-475e-8f4e-6036a85eb461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f32657de-bc34-4792-81fe-6415e1b016a0",
   "metadata": {},
   "source": [
    "Q2. Why are ensemble techniques used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e43dba-047a-484e-b568-836c9aa18e57",
   "metadata": {},
   "source": [
    "Ensemble techniques are used in machine learning for several reasons:\n",
    "\n",
    "Improved Accuracy: Ensemble techniques have the potential to enhance the overall predictive accuracy compared to individual models. By combining the predictions from multiple models, the ensemble can reduce bias, minimize errors, and make more accurate predictions. In many cases, ensembles outperform individual models, especially when the individual models have complementary strengths and weaknesses.\n",
    "\n",
    "Robustness and Stability: Ensembles are often more robust and stable than single models. Individual models may be sensitive to variations in the training data or noise, leading to unstable predictions. Ensembles can mitigate these issues by averaging out the effects of individual model's errors, reducing the impact of outliers, and handling noise more effectively.\n",
    "\n",
    "Reduced Overfitting: Overfitting occurs when a model performs well on the training data but fails to generalize to unseen data. Ensembles can help mitigate overfitting by combining multiple models that are trained on different subsets of the data or using different algorithms. This diversity helps prevent over-reliance on idiosyncrasies in the training set and improves the generalization ability of the ensemble.\n",
    "\n",
    "Capturing Complex Relationships: Ensembles can capture complex relationships in the data more effectively than individual models. By combining different models with different perspectives or using different algorithms, ensembles can capture diverse patterns and uncover hidden relationships that might be missed by a single model. This can be particularly beneficial in complex and high-dimensional datasets.\n",
    "\n",
    "Handling Uncertainty: Ensemble techniques provide a natural way to estimate and handle uncertainty in predictions. By considering multiple models, ensembles can provide not only a point prediction but also a measure of confidence or probability associated with the prediction. This can be useful in decision-making processes where understanding the uncertainty is crucial.\n",
    "\n",
    "Flexibility: Ensemble techniques are flexible and can be applied to various machine learning algorithms and problem domains. Whether it's decision trees, neural networks, support vector machines, or any other algorithm, ensembles can be constructed using a combination of different models. This versatility makes ensemble techniques widely applicable in different contexts.\n",
    "\n",
    "Overall, ensemble techniques offer a powerful approach to improve the performance, stability, and generalization ability of machine learning models. By combining multiple models, ensembles leverage the strengths of individual models while mitigating their weaknesses, leading to more accurate and robust predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a516c42c-f44b-4a71-8789-fb67d47cbbdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a94c096-d342-46cf-b3bc-0e6d8c862330",
   "metadata": {},
   "source": [
    "Q3. What is bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250e7e61-7a80-4de2-898f-12897e79c68f",
   "metadata": {},
   "source": [
    "Bagging, short for bootstrap aggregating, is an ensemble technique in machine learning. It involves creating multiple subsets of the original training dataset through a process called bootstrapping, training separate models on each subset, and combining their predictions to obtain the final prediction.\n",
    "\n",
    "The steps involved in bagging are as follows:\n",
    "\n",
    "Bootstrapping: The bagging process starts by creating multiple subsets, or \"bags,\" of the original training dataset. Each bag is created by randomly sampling the training data with replacement. This means that each bag can contain duplicate instances from the original dataset, and some instances may be omitted. By sampling with replacement, the size of each bag remains the same as the original dataset.\n",
    "\n",
    "Model Training: Once the bags are created, a separate model is trained on each bag. The models can be of any type, such as decision trees, neural networks, or support vector machines. Each model is trained independently on its corresponding bag.\n",
    "\n",
    "Prediction Combination: After training all the models, the predictions from each model are combined to obtain the final prediction. The combination can be done in various ways, depending on the problem type. For regression tasks, the predictions can be averaged across the models, while for classification tasks, voting or averaging probabilities can be used.\n",
    "\n",
    "The key idea behind bagging is that by training models on different subsets of the data and combining their predictions, the ensemble can reduce variance and improve the overall predictive performance. It helps to stabilize the models by reducing the impact of individual instances or noisy samples in the training data.\n",
    "\n",
    "One of the popular bagging algorithms is Random Forest, which uses an ensemble of decision trees. Random Forest combines bagging with random feature selection during the tree-building process to further enhance the diversity and performance of the ensemble.\n",
    "\n",
    "Bagging is particularly effective when individual models tend to overfit the training data or when the dataset is noisy. It is a widely used ensemble technique due to its simplicity, flexibility, and ability to improve the accuracy and robustness of machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722a73a1-69f7-4c40-93b1-b2b373064d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ba8241a-a1dc-416a-849b-2b38a1ec4da8",
   "metadata": {},
   "source": [
    "Q4. What is boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fdd738-9bad-4e10-aaf1-97ca8aaedef5",
   "metadata": {},
   "source": [
    "Boosting is an ensemble technique in machine learning that combines multiple weak or base models to create a strong predictive model. Unlike bagging, where models are trained independently, boosting sequentially trains models, with each subsequent model focusing on improving the mistakes made by the previous models.\n",
    "\n",
    "The general idea behind boosting can be summarized in the following steps:\n",
    "\n",
    "Base Model Training: The boosting process starts by training an initial base model on the original training dataset. This base model can be a simple model that performs slightly better than random guessing, such as a decision tree with limited depth (called a \"weak learner\").\n",
    "\n",
    "Instance Weighting: Each instance in the training data is assigned an initial weight. Initially, all instances are given equal weight, but as the boosting process progresses, the weights are adjusted based on the performance of the previous models. The instances that are misclassified or have higher errors are assigned higher weights, making them more influential in the subsequent model training.\n",
    "\n",
    "Sequential Model Training: The subsequent models are trained iteratively, with each model focusing on the instances that were misclassified or had higher errors in the previous iterations. The model training process emphasizes learning from the mistakes made by the previous models and adjusting the instance weights accordingly.\n",
    "\n",
    "Weighted Voting: Once all the models are trained, the final prediction is made by combining the predictions of all the models using weighted voting. The weights assigned to each model's prediction are typically determined based on their performance during training. Some boosting algorithms also assign weights to each model based on their overall contribution to the ensemble.\n",
    "\n",
    "The boosting process continues until a specified number of models are trained, a predefined performance criterion is met, or no further improvement is observed.\n",
    "\n",
    "Popular boosting algorithms include AdaBoost (Adaptive Boosting) and Gradient Boosting Machines (GBM). AdaBoost adjusts the instance weights to focus on the misclassified instances, while GBM uses gradient descent optimization to iteratively minimize the loss function by adding new models.\n",
    "\n",
    "Boosting is effective in improving the predictive accuracy, especially in situations where weak learners can be combined to create a strong learner. It is particularly useful when dealing with complex datasets and tasks such as classification or regression. Boosting algorithms have been widely adopted in various domains due to their ability to handle high-dimensional data and produce accurate predictions.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4ce416-213a-47c2-b59d-56e675b2c2cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aca24df1-8942-4de1-9259-e88d1ca86649",
   "metadata": {},
   "source": [
    "Q5. What are the benefits of using ensemble techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c74fac-fac0-4deb-a286-e46f45a0694a",
   "metadata": {},
   "source": [
    "Using ensemble techniques in machine learning offers several benefits:\n",
    "\n",
    "Improved Accuracy: Ensemble techniques can significantly improve the predictive accuracy compared to individual models. By combining multiple models, the ensemble can leverage the collective knowledge and reduce errors or biases present in individual models. The ensemble can capture diverse patterns, make more accurate predictions, and provide robust results.\n",
    "\n",
    "Robustness and Stability: Ensembles are generally more robust and stable than single models. Individual models may be sensitive to variations in the training data or noise, leading to unstable predictions. Ensembles can mitigate this issue by averaging out the effects of individual model's errors, reducing the impact of outliers, and handling noise more effectively. This helps in achieving consistent and reliable predictions.\n",
    "\n",
    "Reduced Overfitting: Overfitting occurs when a model performs well on the training data but fails to generalize to unseen data. Ensemble techniques can help mitigate overfitting by combining multiple models that are trained on different subsets of the data or using different algorithms. This diversity helps prevent over-reliance on idiosyncrasies in the training set and improves the generalization ability of the ensemble.\n",
    "\n",
    "Handling Complexity: Ensemble techniques are effective in handling complex patterns and relationships in the data. By combining different models with different perspectives or using different algorithms, ensembles can capture diverse patterns and uncover hidden relationships that might be missed by a single model. This is especially useful in tasks involving high-dimensional data or complex decision boundaries.\n",
    "\n",
    "Uncertainty Estimation: Ensemble techniques provide a natural way to estimate and handle uncertainty in predictions. By considering multiple models, ensembles can provide not only a point prediction but also a measure of confidence or probability associated with the prediction. This can be valuable in decision-making processes where understanding the uncertainty is crucial.\n",
    "\n",
    "Versatility: Ensemble techniques are versatile and can be applied to various machine learning algorithms and problem domains. They are compatible with different types of models, including decision trees, neural networks, support vector machines, etc. This versatility allows ensembles to be used in a wide range of applications, from regression and classification to anomaly detection and recommendation systems.\n",
    "\n",
    "Overall, ensemble techniques offer a powerful approach to improve the performance, stability, and generalization ability of machine learning models. They provide a means to leverage the strengths of individual models while mitigating their weaknesses, resulting in more accurate, robust, and reliable predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2afb4a3-178e-4c2e-bccf-d339c647ae47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d843129b-2fb4-4762-a962-f7a06819b95e",
   "metadata": {},
   "source": [
    "Q6. Are ensemble techniques always better than individual models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db940f7-0ea7-45e1-a5ab-fa39955a98e8",
   "metadata": {},
   "source": [
    "Ensemble techniques are powerful and can often outperform individual models. However, whether ensemble techniques are always better than individual models depends on various factors, including the specific problem, the quality of individual models, the diversity of the ensemble, and the amount and quality of available data.\n",
    "\n",
    "While ensemble techniques offer several advantages, there are scenarios where individual models might be preferable:\n",
    "\n",
    "Simplicity and Interpretability: Individual models are often simpler and more interpretable than ensemble models. If interpretability is a crucial requirement, such as in legal or regulatory settings, a single model may be preferred over an ensemble, as it allows for easier understanding and explanation of the decision-making process.\n",
    "\n",
    "Limited Data: Ensembles typically require a sufficient amount of diverse data to train multiple models effectively. In cases where the available data is limited, training multiple models may lead to overfitting or insufficient performance improvement. In such situations, it may be more effective to focus on improving a single model using feature engineering, regularization techniques, or model tuning.\n",
    "\n",
    "Computation and Resource Constraints: Ensembles can be computationally intensive and require more resources compared to individual models. If there are constraints on computation time or available resources, using a single model may be a more practical choice. Training and maintaining an ensemble may not be feasible in resource-constrained environments.\n",
    "\n",
    "High-Quality Individual Model: In some cases, a single model may already perform exceptionally well on its own. If the individual model achieves a high level of accuracy and generalization, the improvement gained from an ensemble may be marginal. In such situations, the additional complexity introduced by ensemble techniques may not be justified.\n",
    "\n",
    "It's important to note that the performance of ensemble techniques depends on the quality and diversity of the individual models. If the individual models are weak or highly correlated, the ensemble may not provide significant performance improvements. Therefore, it's essential to carefully consider the characteristics of the problem and the available models when deciding whether to use ensemble techniques or rely on individual models.\n",
    "\n",
    "Ultimately, the choice between using ensemble techniques or individual models depends on a trade-off between performance improvement, interpretability, available resources, and the specific requirements of the problem at hand.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479ee712-f001-46cf-b1d9-c92d2896c160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08832a56-67fc-4296-bbfb-e5f695654d76",
   "metadata": {},
   "source": [
    "Q7. How is the confidence interval calculated using bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200b412d-bba6-4e18-a6fd-2a26bdc74976",
   "metadata": {},
   "source": [
    "The confidence interval can be calculated using bootstrap resampling, a technique that estimates the uncertainty associated with a statistic or parameter by resampling the original dataset. The steps to calculate a bootstrap confidence interval are as follows:\n",
    "\n",
    "Data Resampling: Randomly sample the original dataset with replacement to create a bootstrap sample. The bootstrap sample has the same size as the original dataset but may contain duplicate instances and exclude some instances from the original dataset. This resampling process is repeated multiple times (e.g., thousands of iterations) to create a set of bootstrap samples.\n",
    "\n",
    "Statistic Calculation: Calculate the desired statistic or parameter of interest on each bootstrap sample. The statistic could be a mean, median, standard deviation, or any other measure that characterizes the data.\n",
    "\n",
    "Bootstrap Distribution: Collect the calculated statistics from all bootstrap samples to create a bootstrap distribution. This distribution represents the variation in the statistic due to random sampling from the original dataset. It provides an estimate of the sampling distribution of the statistic.\n",
    "\n",
    "Confidence Interval Estimation: Calculate the confidence interval based on the bootstrap distribution. The confidence interval defines a range of values within which the true parameter or statistic is likely to fall with a certain level of confidence. The most common approach is to use the percentiles of the bootstrap distribution to determine the lower and upper bounds of the confidence interval. For example, a 95% confidence interval corresponds to the 2.5th and 97.5th percentiles of the bootstrap distribution.\n",
    "\n",
    "By repeatedly resampling the dataset and estimating the statistic, the bootstrap method provides an empirical approximation of the sampling distribution. It accounts for the variability in the data and allows for quantifying the uncertainty associated with the estimated statistic or parameter. The confidence interval obtained through bootstrap provides a range of plausible values for the true population parameter, along with an associated level of confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14f6159-4cf7-4058-948f-a71c9ce49862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c316611-f62e-40ef-b880-bc7ec7beaafe",
   "metadata": {},
   "source": [
    "Q8. How does bootstrap work and What are the steps involved in bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df69c951-230c-4917-9881-dc76f983dcaa",
   "metadata": {},
   "source": [
    "Bootstrap is a resampling technique that estimates the sampling distribution of a statistic by creating multiple resamples from the original dataset. It allows for assessing the variability and uncertainty associated with a statistic or parameter without making strong assumptions about the underlying population distribution. The steps involved in the bootstrap method are as follows:\n",
    "\n",
    "1. Data Resampling: Start with the original dataset of size N. Randomly sample N instances from the dataset with replacement, which means each instance in the resample can be selected multiple times or not at all. This process creates a bootstrap sample, which has the same size as the original dataset but may contain duplicate instances and exclude some instances.\n",
    "\n",
    "2. Statistic Calculation: Calculate the desired statistic or parameter of interest on the bootstrap sample. The statistic could be a mean, median, standard deviation, correlation, or any other measure that characterizes the data. This step involves performing the same computation as you would on the original dataset.\n",
    "\n",
    "3. Repeat Steps 1 and 2: Repeat steps 1 and 2 a large number of times (typically thousands of iterations) to create multiple bootstrap samples and calculate the statistic for each sample. Each bootstrap sample is obtained by resampling from the original dataset, and the statistic is calculated for each sample.\n",
    "\n",
    "4. Bootstrap Distribution: Collect the calculated statistics from all bootstrap samples to create the bootstrap distribution. This distribution represents the variation in the statistic due to random sampling from the original dataset. It provides an empirical approximation of the sampling distribution of the statistic.\n",
    "\n",
    "5. Estimating Uncertainty: Use the bootstrap distribution to estimate the uncertainty associated with the statistic or parameter. This can be done by calculating various summary statistics of the bootstrap distribution, such as the mean, standard deviation, percentiles, or confidence intervals.\n",
    "\n",
    "The bootstrap method provides a way to approximate the sampling distribution of a statistic based on the observed data. By resampling the dataset, it accounts for the variability in the data and allows for quantifying the uncertainty associated with the estimated statistic or parameter. The more iterations of resampling are performed, the more accurate the bootstrap estimates become.\n",
    "\n",
    "Bootstrap is widely used for hypothesis testing, estimation, and constructing confidence intervals. It is particularly valuable when the underlying distribution is unknown or difficult to model, and it provides a non-parametric approach to assess the variability and uncertainty in data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d7f62d-ce59-43fd-9b7e-79b9737e653d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "becd1770-c3c5-4353-8fdd-42f70a2f5e3c",
   "metadata": {},
   "source": [
    "Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a\n",
    "sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use\n",
    "bootstrap to estimate the 95% confidence interval for the population mean height."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032c1494-6bbc-4f21-b7e4-1c4a2f86c0b6",
   "metadata": {},
   "source": [
    "To estimate the 95% confidence interval for the population mean height using the bootstrap method, we can follow these steps:\n",
    "\n",
    "Original Sample: Start with the original sample of 50 tree heights and calculate the sample mean and standard deviation. Given that the sample mean is 15 meters and the sample standard deviation is 2 meters.\n",
    "\n",
    "Bootstrap Resampling: Perform the bootstrap resampling by randomly selecting 50 heights from the original sample with replacement. Repeat this process a large number of times, such as 10,000 iterations, to create a set of bootstrap samples.\n",
    "\n",
    "Statistic Calculation: Calculate the sample mean for each bootstrap sample.\n",
    "\n",
    "Bootstrap Distribution: Collect the sample means from all bootstrap samples to create the bootstrap distribution.\n",
    "\n",
    "Confidence Interval Estimation: Calculate the 2.5th and 97.5th percentiles of the bootstrap distribution. These percentiles correspond to the lower and upper bounds of the 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "638cbd7e-7b74-4894-bfd5-0b7b5303e9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval: [15.00, 15.00]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original sample information\n",
    "original_sample = np.array([15] * 50)  # The given mean of 15 meters repeated 50 times\n",
    "sample_mean = np.mean(original_sample)\n",
    "sample_std = np.std(original_sample)\n",
    "\n",
    "# Number of bootstrap iterations\n",
    "bootstrap_iterations = 10000\n",
    "\n",
    "# Bootstrap resampling and calculation of sample means\n",
    "bootstrap_sample_means = []\n",
    "for _ in range(bootstrap_iterations):\n",
    "    bootstrap_sample = np.random.choice(original_sample, size=50, replace=True)\n",
    "    bootstrap_sample_mean = np.mean(bootstrap_sample)\n",
    "    bootstrap_sample_means.append(bootstrap_sample_mean)\n",
    "\n",
    "# Bootstrap distribution and confidence interval\n",
    "bootstrap_distribution = np.array(bootstrap_sample_means)\n",
    "lower_bound = np.percentile(bootstrap_distribution, 2.5)\n",
    "upper_bound = np.percentile(bootstrap_distribution, 97.5)\n",
    "\n",
    "# Output the confidence interval\n",
    "print(\"95% Confidence Interval: [{:.2f}, {:.2f}]\".format(lower_bound, upper_bound))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b351a84-e539-4177-be09-6a0a0359a40e",
   "metadata": {},
   "source": [
    "The resulting output will provide the 95% confidence interval for the population mean height based on the bootstrap estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f021053-2f78-49a7-ab9c-3da8de3157ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
